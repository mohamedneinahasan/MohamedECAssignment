{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.                                               Create a white box neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedneinahasan/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               2816      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35841 (140.00 KB)\n",
      "Trainable params: 35841 (140.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define a modified feedforward neural network\n",
    "def create_white_box_nn_modified(input_shape):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "\n",
    "    # Hidden layers (modified)\n",
    "    model.add(layers.Dense(units=256, activation='elu'))  # Increased units and changed activation\n",
    "    model.add(layers.Dense(units=128, activation='selu'))  # Changed activation\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Specify input shape (assuming a binary classification task with input features of size 10)\n",
    "input_shape = (10,)\n",
    "\n",
    "# Create the modified white-box neural network\n",
    "white_box_nn_modified = create_white_box_nn_modified(input_shape)\n",
    "\n",
    "# Display the model summary\n",
    "white_box_nn_modified.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.                                                         Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result using math module: 0.8807970779778823\n",
      "Result using NumPy: [0.88079708]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Using numpy for vectorized operations\n",
    "def sigmoid_np(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example usage:\n",
    "result_math = sigmoid(2.0)\n",
    "result_np = sigmoid_np(np.array([2.0]))\n",
    "\n",
    "print(\"Result using math module:\", result_math)\n",
    "print(\"Result using NumPy:\", result_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tanh function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result using math module: 0.9640275800758169\n",
      "Result using NumPy: [0.96402758]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(2*x) - 1) / (np.exp(2*x) + 1)\n",
    "\n",
    "# Using numpy for vectorized operations\n",
    "def tanh_np(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Example usage:\n",
    "result_math = tanh(2.0)\n",
    "result_np = tanh_np(np.array([2.0]))\n",
    "\n",
    "print(\"Result using math module:\", result_math)\n",
    "print(\"Result using NumPy:\", result_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.RELU function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result using manual implementation: 2.0\n",
      "Result using NumPy: [2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Using numpy for vectorized operations\n",
    "def relu_np(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Example usage:\n",
    "result_math = relu(2.0)\n",
    "result_np = relu_np(np.array([2.0]))\n",
    "\n",
    "print(\"Result using manual implementation:\", result_math)\n",
    "print(\"Result using NumPy:\", result_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.ELU function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.86466472 -0.63212056  0.          1.          2.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = elu(x_values, alpha=1.0)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.PRELU function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02 -0.01  0.    1.    2.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return np.maximum(0, x) + self.alpha * np.minimum(0, x)\n",
    "\n",
    "# Example usage\n",
    "prelu_activation = PReLU(alpha=0.01)\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = prelu_activation(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Leaky RELU function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02 -0.01  0.    1.    2.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = leaky_relu(x_values, alpha=0.01)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.SELU function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.52016209 -1.11132754  0.          1.0507      2.1014    ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def selu(x, scale=1.0507, alpha=1.67326):\n",
    "    return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = selu(x_values)\n",
    "print(output_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Softsign function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.66666667 -0.5         0.          0.5         0.66666667]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def softsign(x):\n",
    "    return x / (1 + np.abs(x))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = softsign(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Softplus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12692801 0.31326169 0.69314718 1.31326169 2.12692801]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = softplus(x_values)\n",
    "print(output_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Hard sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.16666666 0.33333333 0.5        0.66666667 0.83333334\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "def hard_sigmoid(x):\n",
    "    return np.clip(0.16666667 * x + 0.5, 0, 1)\n",
    "# Example usage\n",
    "x_values = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n",
    "output_values = hard_sigmoid(x_values)\n",
    "print(output_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.Swish function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23840584 -0.26894142  0.          0.73105858  1.76159416]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def swish(x):\n",
    "    return x * (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = swish(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.Mish function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.25250148 -0.30340146  0.          0.86509839  1.94395896]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mish(x):\n",
    "    return x * np.tanh(np.log(1 + np.exp(x)))\n",
    "\n",
    "# Example usage\n",
    "x_values = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output_values = mish(x_values)\n",
    "print(output_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python code for chain rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f'(x) = 2*x*cos(x**2)\n",
      "g'(x) = 2*x\n",
      "Chain Rule Result: 2*x*cos(x**2)\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbols\n",
    "x = sp.symbols('x')\n",
    "\n",
    "# Define the inner and outer functions\n",
    "g = x**2\n",
    "f = sp.sin(g)\n",
    "\n",
    "# Compute the derivatives\n",
    "g_prime = sp.diff(g, x)\n",
    "f_prime = sp.diff(f, x)\n",
    "\n",
    "# Apply the chain rule\n",
    "chain_rule_result = sp.diff(f.subs(g, x**2), x)\n",
    "\n",
    "# Print the result\n",
    "print(\"f'(x) =\", f_prime)\n",
    "print(\"g'(x) =\", g_prime)\n",
    "print(\"Chain Rule Result:\", chain_rule_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to perform back propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.14159479453221988\n",
      "Epoch 1000, Loss: 0.12344689570690472\n",
      "Epoch 2000, Loss: 0.11155406432451556\n",
      "Epoch 3000, Loss: 0.07778517861398407\n",
      "Epoch 4000, Loss: 0.03281496018045563\n",
      "Epoch 5000, Loss: 0.01479961501423141\n",
      "Epoch 6000, Loss: 0.008925515193170449\n",
      "Epoch 7000, Loss: 0.006249858172555918\n",
      "Epoch 8000, Loss: 0.004750368741015834\n",
      "Epoch 9000, Loss: 0.0037992877772672767\n",
      "\n",
      "Predictions:\n",
      "[[0.09620443]\n",
      " [0.92303163]\n",
      " [0.92312117]\n",
      " [0.06383034]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return 0.5 * np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mean_squared_error_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "    weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "    return weights_input_hidden, weights_hidden_output\n",
    "\n",
    "def forward_propagation(X, weights_input_hidden, weights_hidden_output):\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "    return hidden_layer_output, output_layer_output\n",
    "\n",
    "def backward_propagation(X, y, hidden_layer_output, output_layer_output, \n",
    "                         weights_input_hidden, weights_hidden_output, learning_rate):\n",
    "    output_error = mean_squared_error_derivative(y, output_layer_output)\n",
    "    output_delta = output_error * sigmoid_derivative(output_layer_output)\n",
    "\n",
    "    hidden_error = output_delta.dot(weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    weights_hidden_output -= hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "    weights_input_hidden -= X.T.dot(hidden_delta) * learning_rate\n",
    "\n",
    "def train_neural_network(X, y, hidden_size, epochs, learning_rate):\n",
    "    input_size = X.shape[1]\n",
    "    output_size = 1\n",
    "\n",
    "    weights_input_hidden, weights_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        hidden_layer_output, output_layer_output = forward_propagation(X, weights_input_hidden, weights_hidden_output)\n",
    "\n",
    "        backward_propagation(X, y, hidden_layer_output, output_layer_output, \n",
    "                             weights_input_hidden, weights_hidden_output, learning_rate)\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            loss = mean_squared_error(y, output_layer_output)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return weights_input_hidden, weights_hidden_output\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "trained_weights_input_hidden, trained_weights_hidden_output = train_neural_network(X, y, hidden_size=4, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "# Test the trained network\n",
    "_, predictions = forward_propagation(X, trained_weights_input_hidden, trained_weights_hidden_output)\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation for ant colony optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Best Fitness: -1.0035493852035713\n",
      "Iteration 1, Best Fitness: -1.0086646997222268\n",
      "Iteration 2, Best Fitness: -1.0114553734563616\n",
      "Iteration 3, Best Fitness: -1.0061845416066555\n",
      "Iteration 4, Best Fitness: -1.0050921433343987\n",
      "Iteration 5, Best Fitness: -1.0078150249087314\n",
      "Iteration 6, Best Fitness: -1.002436433718481\n",
      "Iteration 7, Best Fitness: -1.006885871098603\n",
      "Iteration 8, Best Fitness: -1.0088225666852066\n",
      "Iteration 9, Best Fitness: -1.010326026385033\n",
      "Iteration 10, Best Fitness: -1.0093474017228852\n",
      "Iteration 11, Best Fitness: -1.0111540742910003\n",
      "Iteration 12, Best Fitness: -1.0019213711315988\n",
      "Iteration 13, Best Fitness: -1.008733668281065\n",
      "Iteration 14, Best Fitness: -1.0095979055132498\n",
      "Iteration 15, Best Fitness: -1.0060910984575047\n",
      "Iteration 16, Best Fitness: -1.0119466457955895\n",
      "Iteration 17, Best Fitness: -1.0166604296892574\n",
      "Iteration 18, Best Fitness: -1.0090508291589484\n",
      "Iteration 19, Best Fitness: -1.0051472001094401\n",
      "Iteration 20, Best Fitness: -1.0021259690087454\n",
      "Iteration 21, Best Fitness: -1.009473731769719\n",
      "Iteration 22, Best Fitness: -1.0095879282161597\n",
      "Iteration 23, Best Fitness: -1.0065054083595877\n",
      "Iteration 24, Best Fitness: -1.0152674538327344\n",
      "Iteration 25, Best Fitness: -1.0062749994657691\n",
      "Iteration 26, Best Fitness: -1.0082170196422178\n",
      "Iteration 27, Best Fitness: -1.012040928924381\n",
      "Iteration 28, Best Fitness: -1.0085602137383407\n",
      "Iteration 29, Best Fitness: -1.0080776363917168\n",
      "Iteration 30, Best Fitness: -1.003393730122676\n",
      "Iteration 31, Best Fitness: -1.0045936035107714\n",
      "Iteration 32, Best Fitness: -1.010847160523176\n",
      "Iteration 33, Best Fitness: -1.0048733277750277\n",
      "Iteration 34, Best Fitness: -1.0110208081885097\n",
      "Iteration 35, Best Fitness: -1.007694758525846\n",
      "Iteration 36, Best Fitness: -1.0058278211501395\n",
      "Iteration 37, Best Fitness: -1.0064517397426356\n",
      "Iteration 38, Best Fitness: -1.0115611230474677\n",
      "Iteration 39, Best Fitness: -1.0052338870532123\n",
      "Iteration 40, Best Fitness: -1.0061109841485305\n",
      "Iteration 41, Best Fitness: -1.0059387834071791\n",
      "Iteration 42, Best Fitness: -1.0078610022073877\n",
      "Iteration 43, Best Fitness: -1.0072238061482528\n",
      "Iteration 44, Best Fitness: -1.0064252705452026\n",
      "Iteration 45, Best Fitness: -1.0107586709739051\n",
      "Iteration 46, Best Fitness: -1.0040112629379034\n",
      "Iteration 47, Best Fitness: -1.0005860702172986\n",
      "Iteration 48, Best Fitness: -1.0072039109521354\n",
      "Iteration 49, Best Fitness: -1.013247903464738\n",
      "Iteration 50, Best Fitness: -1.0064993372733877\n",
      "Iteration 51, Best Fitness: -1.0141629949786284\n",
      "Iteration 52, Best Fitness: -1.0062156371923399\n",
      "Iteration 53, Best Fitness: -1.0039989082745637\n",
      "Iteration 54, Best Fitness: -1.0070974838346731\n",
      "Iteration 55, Best Fitness: -1.0050581072450573\n",
      "Iteration 56, Best Fitness: -1.0112213060331334\n",
      "Iteration 57, Best Fitness: -1.0128301584133017\n",
      "Iteration 58, Best Fitness: -1.0108617868486847\n",
      "Iteration 59, Best Fitness: -1.0074572484391826\n",
      "Iteration 60, Best Fitness: -1.008546110347373\n",
      "Iteration 61, Best Fitness: -1.00793302281851\n",
      "Iteration 62, Best Fitness: -1.0034530808493274\n",
      "Iteration 63, Best Fitness: -1.007004403973672\n",
      "Iteration 64, Best Fitness: -1.0059510106354022\n",
      "Iteration 65, Best Fitness: -1.0049748604981188\n",
      "Iteration 66, Best Fitness: -1.0063644710599156\n",
      "Iteration 67, Best Fitness: -1.0050111676288105\n",
      "Iteration 68, Best Fitness: -1.003639341727591\n",
      "Iteration 69, Best Fitness: -1.0016209505395777\n",
      "Iteration 70, Best Fitness: -1.0059085986051104\n",
      "Iteration 71, Best Fitness: -1.0034427538153183\n",
      "Iteration 72, Best Fitness: -1.006394253396564\n",
      "Iteration 73, Best Fitness: -1.0075816550095633\n",
      "Iteration 74, Best Fitness: -1.00748620582688\n",
      "Iteration 75, Best Fitness: -1.0033705491483889\n",
      "Iteration 76, Best Fitness: -1.0046558156337169\n",
      "Iteration 77, Best Fitness: -1.0100220771916941\n",
      "Iteration 78, Best Fitness: -1.004838922827409\n",
      "Iteration 79, Best Fitness: -1.0066443467726036\n",
      "Iteration 80, Best Fitness: -1.0070204194929488\n",
      "Iteration 81, Best Fitness: -1.006807490054588\n",
      "Iteration 82, Best Fitness: -1.0084669300536488\n",
      "Iteration 83, Best Fitness: -1.0151802515932682\n",
      "Iteration 84, Best Fitness: -1.0076568189656043\n",
      "Iteration 85, Best Fitness: -1.0052838195358116\n",
      "Iteration 86, Best Fitness: -1.0055953539242022\n",
      "Iteration 87, Best Fitness: -1.0059096203520206\n",
      "Iteration 88, Best Fitness: -1.0076343889211468\n",
      "Iteration 89, Best Fitness: -1.0056405776624713\n",
      "Iteration 90, Best Fitness: -1.0112279238397481\n",
      "Iteration 91, Best Fitness: -1.0044672416834037\n",
      "Iteration 92, Best Fitness: -1.0081430379942433\n",
      "Iteration 93, Best Fitness: -1.0094160155679763\n",
      "Iteration 94, Best Fitness: -1.0095515855024373\n",
      "Iteration 95, Best Fitness: -1.0020737616499618\n",
      "Iteration 96, Best Fitness: -1.0087676451687668\n",
      "Iteration 97, Best Fitness: -1.013388977976891\n",
      "Iteration 98, Best Fitness: -1.0053346961244727\n",
      "Iteration 99, Best Fitness: -1.0067023479646922\n",
      "\n",
      "Final Predictions:\n",
      "[array([0.541049]), array([0.54277578]), array([0.54270162]), array([0.5444194])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the task (e.g., XOR)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# ACO parameters\n",
    "num_ants = 5\n",
    "pheromone_matrix = np.ones((input_size * hidden_size + hidden_size * output_size,))\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the neural network forward propagation\n",
    "def forward_propagation(weights, X):\n",
    "    input_hidden_weights = weights[:input_size * hidden_size].reshape(input_size, hidden_size)\n",
    "    hidden_output_weights = weights[input_size * hidden_size:].reshape(hidden_size, output_size)\n",
    "\n",
    "    hidden_layer_input = np.dot(X, input_hidden_weights)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_output, hidden_output_weights)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "    return output_layer_output\n",
    "\n",
    "# Define the fitness function based on mean squared error\n",
    "def fitness(weights):\n",
    "    total_error = 0\n",
    "    for xi, target in zip(X, y):\n",
    "        output = forward_propagation(weights, xi)\n",
    "        total_error += np.mean((output - target) ** 2)\n",
    "    return -total_error  # Maximizing fitness, so negative of error\n",
    "\n",
    "# ACO-inspired exploration\n",
    "def explore_weights(pheromone_matrix):\n",
    "    weights = np.random.rand(len(pheromone_matrix))\n",
    "    weights = weights * pheromone_matrix\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "# Training loop\n",
    "num_iterations = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    for ant in range(num_ants):\n",
    "        # Explore the weight space using ACO\n",
    "        candidate_weights = explore_weights(pheromone_matrix)\n",
    "\n",
    "        # Evaluate fitness\n",
    "        fitness_candidate = fitness(candidate_weights)\n",
    "\n",
    "        # Update pheromone based on fitness\n",
    "        pheromone_matrix += learning_rate * (fitness_candidate - pheromone_matrix)\n",
    "\n",
    "    # Print the best fitness in each iteration\n",
    "    best_fitness = max(fitness(candidate_weights) for ant in range(num_ants))\n",
    "    print(f\"Iteration {iteration}, Best Fitness: {best_fitness}\")\n",
    "\n",
    "# Get the best weights from the pheromone matrix\n",
    "best_weights = explore_weights(pheromone_matrix)\n",
    "\n",
    "# Test the best weights\n",
    "predictions = [forward_propagation(best_weights, xi) for xi in X]\n",
    "print(\"\\nFinal Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation for genetic algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 9 into shape (4,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Genetic algorithm loop\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_generations):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Evaluate fitness for each individual in the population\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [fitness(individual) \u001b[38;5;28;01mfor\u001b[39;00m individual \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Select parents based on fitness scores (roulette wheel selection)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     parents_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(population_size), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, p\u001b[38;5;241m=\u001b[39mfitness_scores \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(fitness_scores))\n",
      "Cell \u001b[0;32mIn[11], line 54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Genetic algorithm loop\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_generations):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Evaluate fitness for each individual in the population\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[43mfitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m individual \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Select parents based on fitness scores (roulette wheel selection)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     parents_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(population_size), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, p\u001b[38;5;241m=\u001b[39mfitness_scores \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(fitness_scores))\n",
      "Cell \u001b[0;32mIn[11], line 47\u001b[0m, in \u001b[0;36mfitness\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     45\u001b[0m total_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, y):\n\u001b[0;32m---> 47\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     total_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((output \u001b[38;5;241m-\u001b[39m target) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mtotal_error\n",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m, in \u001b[0;36mforward_propagation\u001b[0;34m(weights, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(weights, X):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Extract weights for input to hidden layer and hidden to output layer\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     input_hidden_weights \u001b[38;5;241m=\u001b[39m weights[:input_size \u001b[38;5;241m*\u001b[39m hidden_size]\u001b[38;5;241m.\u001b[39mreshape(input_size, hidden_size)\n\u001b[0;32m---> 31\u001b[0m     hidden_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Input to hidden layer\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     hidden_layer_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, input_hidden_weights)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 9 into shape (4,1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the task (e.g., XOR)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Define genetic algorithm parameters\n",
    "population_size = 10\n",
    "mutation_rate = 0.1\n",
    "num_generations = 100\n",
    "\n",
    "# Initialize the population with random weights\n",
    "population = [np.random.randn((input_size + 1) * hidden_size + (hidden_size + 1) * output_size) for _ in range(population_size)]\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the neural network forward propagation\n",
    "def forward_propagation(weights, X):\n",
    "    # Extract weights for input to hidden layer and hidden to output layer\n",
    "    input_hidden_weights = weights[:input_size * hidden_size].reshape(input_size, hidden_size)\n",
    "    hidden_output_weights = weights[input_size * hidden_size:].reshape(hidden_size, output_size)\n",
    "\n",
    "    # Input to hidden layer\n",
    "    hidden_layer_input = np.dot(X, input_hidden_weights)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    # Hidden to output layer\n",
    "    output_layer_input = np.dot(hidden_layer_output, hidden_output_weights)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "    return output_layer_output\n",
    "\n",
    "# Define the fitness function based on mean squared error\n",
    "def fitness(weights):\n",
    "    total_error = 0\n",
    "    for xi, target in zip(X, y):\n",
    "        output = forward_propagation(weights, xi)\n",
    "        total_error += np.mean((output - target) ** 2)\n",
    "    return -total_error  # Maximizing fitness, so negative of error\n",
    "\n",
    "# Genetic algorithm loop\n",
    "for generation in range(num_generations):\n",
    "    # Evaluate fitness for each individual in the population\n",
    "    fitness_scores = [fitness(individual) for individual in population]\n",
    "\n",
    "    # Select parents based on fitness scores (roulette wheel selection)\n",
    "    parents_indices = np.random.choice(range(population_size), size=2, p=fitness_scores / np.sum(fitness_scores))\n",
    "\n",
    "    # Crossover (single-point crossover)\n",
    "    crossover_point = np.random.randint(len(population[0]))\n",
    "    child1 = np.concatenate((population[parents_indices[0]][:crossover_point], population[parents_indices[1]][crossover_point:]))\n",
    "    child2 = np.concatenate((population[parents_indices[1]][:crossover_point], population[parents_indices[0]][crossover_point:]))\n",
    "\n",
    "    # Mutation\n",
    "    child1[np.random.rand(len(child1)) < mutation_rate] += np.random.randn(sum(np.random.rand(len(child1)) < mutation_rate))\n",
    "    child2[np.random.rand(len(child2)) < mutation_rate] += np.random.randn(sum(np.random.rand(len(child2)) < mutation_rate))\n",
    "\n",
    "    # Replace the least fit individuals with the new children\n",
    "    least_fit_index = np.argmin(fitness_scores)\n",
    "    population[least_fit_index] = child1\n",
    "    population[(least_fit_index + 1) % population_size] = child2\n",
    "\n",
    "    # Print the best fitness in each generation\n",
    "    best_fitness = max(fitness_scores)\n",
    "    print(f\"Generation {generation}, Best Fitness: {best_fitness}\")\n",
    "\n",
    "# Get the best individual (weights) from the final population\n",
    "best_weights = population[np.argmax(fitness_scores)]\n",
    "\n",
    "# Test the best individual\n",
    "predictions = [forward_propagation(best_weights, xi) for xi in X]\n",
    "print(\"\\nFinal Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation for cultural algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.014289272308712329\n",
      "Epoch 1000, Loss: 0.011226292092449113\n",
      "Epoch 2000, Loss: 0.009218060275883028\n",
      "Epoch 3000, Loss: 0.007016370857863089\n",
      "Epoch 4000, Loss: 0.0020108103191735756\n",
      "Epoch 5000, Loss: 0.0003612079458874026\n",
      "Epoch 6000, Loss: 0.0001293679082017331\n",
      "Epoch 7000, Loss: 6.459424445205113e-05\n",
      "Epoch 8000, Loss: 3.825477867909053e-05\n",
      "Epoch 9000, Loss: 2.5125083433008495e-05\n",
      "\n",
      "Final Output:\n",
      "[[0.0741223 ]\n",
      " [0.94842511]\n",
      " [0.92885169]\n",
      " [0.06584961]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "    weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "    return weights_input_hidden, weights_hidden_output\n",
    "\n",
    "def forward_propagation(X, weights_input_hidden, weights_hidden_output):\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden)\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "    output_layer_output = sigmoid(output_layer_input)\n",
    "    return hidden_layer_output, output_layer_output\n",
    "\n",
    "def backpropagation(X, y, hidden_layer_output, output_layer_output, weights_hidden_output):\n",
    "    output_error = y - output_layer_output\n",
    "    output_delta = output_error * sigmoid_derivative(output_layer_output)\n",
    "    hidden_error = output_delta.dot(weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer_output)\n",
    "    return output_delta, hidden_delta\n",
    "\n",
    "def update_weights(X, hidden_layer_output, output_delta, weights_input_hidden, weights_hidden_output, learning_rate):\n",
    "    weights_hidden_output += hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "    weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "    return weights_input_hidden, weights_hidden_output\n",
    "\n",
    "# Define the task (e.g., XOR)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Initialize weights\n",
    "weights_input_hidden, weights_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    hidden_layer_output, output_layer_output = forward_propagation(X, weights_input_hidden, weights_hidden_output)\n",
    "\n",
    "    # Backpropagation\n",
    "    output_delta, hidden_delta = backpropagation(X, y, hidden_layer_output, output_layer_output, weights_hidden_output)\n",
    "\n",
    "    # Update weights\n",
    "    weights_input_hidden, weights_hidden_output = update_weights(X, hidden_layer_output, output_delta, weights_input_hidden, weights_hidden_output, learning_rate)\n",
    "\n",
    "    # Print the loss every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(output_delta))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Test the trained network\n",
    "_, final_output = forward_propagation(X, weights_input_hidden, weights_hidden_output)\n",
    "print(\"\\nFinal Output:\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
